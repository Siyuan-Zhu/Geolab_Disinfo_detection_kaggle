{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809b5e59",
   "metadata": {},
   "source": [
    "# This is the example code from the competition website\n",
    "https://danrunfola.gitbook.io/datasec-challenge/phase-1/implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8112c1f",
   "metadata": {},
   "source": [
    "# Installing Libraries\n",
    "We have to activate conda environment before installing any libraries. There are bunch of libraries we need to install like pandas,numpy,sklearn,nltk, pytorch and transformers.\n",
    "Once you've activated your environment, you can install any packages using standard commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For pandas,numpy and sklearn:\n",
    "\n",
    "\n",
    "# this should be changed\n",
    "conda install pandas\n",
    "conda install numpy\n",
    "conda install scikit-learn\n",
    "\n",
    "#For nltk:\n",
    "conda install -c anaconda nltk\n",
    "\n",
    "#For tqdm:\n",
    "conda install -c conda-forge tqdm\n",
    "\n",
    "#For pytorch:\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.6 -c pytorch -c nvidia(for both cpu and gpu)\n",
    "\n",
    "#For transformers:\n",
    "conda install -c huggingface transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e26791e",
   "metadata": {},
   "source": [
    "## The Python Script\n",
    "Note that i am using BERT-12 Layer algorithm for classification but you can try with any algorithm you want. We have two scripts: a train script and a test script. In the train script, the model is trained using the traindataset, and in the test script, the trained model is loaded and a test dataset is provided for label prediction.\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f0efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importing all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import sys\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#Loading the data\n",
    "Data=pd.read_csv(\"Give path to the csv train dataset file you downloaded\")\n",
    "Data=Data.iloc[:,1:](Keeping only text and label columns)\n",
    "Data\n",
    "\n",
    "#Checking and removing null values\n",
    "for col in Data.columns:\n",
    "   print(col, Data[col].isnull().sum())\n",
    "Data= Data.dropna()\n",
    "\n",
    "#Now we perform some data preprocessing tasks.\n",
    "#Converting text to lowercase\n",
    "Data['clean_text'] = Data.full_text.str.lower()\n",
    "\n",
    "#Removing urls\n",
    "Data.clean_text = Data.clean_text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "Data.clean_text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x)) \n",
    "\n",
    "#Removing non-letter characters\n",
    "Data.clean_text = Data.clean_text.apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
    "Data.clean_text = Data.clean_text.apply(lambda x: re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];=\\'\\\\#\\\\]\", '', x))\n",
    "\n",
    "#Removing hashtag and quote symbols\n",
    "Data.clean_text = Data.clean_text.apply(lambda x: re.sub(r'#', '', x))\n",
    "Data.clean_text = Data.clean_text.apply(lambda x: re.sub(r\"'\", '', x))\n",
    "\n",
    "#Tokenization\n",
    "tknzr = TweetTokenizer()\n",
    "Data['clean_text'] = Data['clean_text'].apply(tknzr.tokenize)\n",
    "\n",
    "#Removing Punctuations\n",
    "PUNCUATION_LIST = list(string.punctuation)\n",
    "def remove_punctuation(word_list):\n",
    "   \"\"\"Remove punctuation tokens from a list of tokens\"\"\"\n",
    "   return [w for w in word_list if w not in PUNCUATION_LIST]\n",
    "Data['clean_text'] = Data['clean_text'].apply(remove_punctuation)\n",
    "\n",
    "#Removing Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "Data['clean_text'] = Data['clean_text'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "#Performing Lemmatization\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = TweetTokenizer()\n",
    "def lemmatize_text(text):\n",
    "return [(lemmatizer.lemmatize(w)) for w in text]\n",
    "Data['clean_text'] = Data['clean_text'].apply(lemmatize_text)\n",
    "\n",
    "#Putting tokens back into string\n",
    "Data['clean_text']=[' '.join(map(str,l)) for l in Data['clean_text']]\n",
    "\n",
    "#Labeling original as '0' and Fake as '1'\n",
    "Data.Y[Data.Y == 'Original'] = 0\n",
    "Data.Y[Data.Y == 'Fake'] = 1\n",
    "TrainData=Data.iloc[:,1:]\n",
    "\n",
    "# Get the lists of sentences and their labels.\n",
    "sentences = Traindata.clean_text.values\n",
    "labels = Traindata.Y.values\n",
    "labels = np.asarray(labels).astype(np.float32)\n",
    "\n",
    "#Importing the Bert Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "   # `encode_plus` will:\n",
    "   #   (1) Tokenize the sentence.\n",
    "   #   (2) Prepend the `[CLS]` token to the start.\n",
    "   #   (3) Append the `[SEP]` token to the end.\n",
    "   #   (4) Map tokens to their IDs.\n",
    "   #   (5) Pad or truncate the sentence to `max_length`\n",
    "   #   (6) Create attention masks for [PAD] tokens.\n",
    "   encoded_dict = tokenizer.encode_plus(\n",
    "                       sent,                      # Sentence to encode.\n",
    "                       add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                       max_length = 64,           # Pad & truncate all sentences.\n",
    "                       pad_to_max_length = True,\n",
    "                       return_attention_mask = True,   # Construct attn. masks.\n",
    "                       return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                  )\n",
    "   \n",
    "   # Add the encoded sentence to the list.    \n",
    "   input_ids.append(encoded_dict['input_ids'])\n",
    "   \n",
    "   # And its attention mask (simply differentiates padding from non-padding).\n",
    "   attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "           train_dataset,  # The training samples.\n",
    "           sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "           batch_size = batch_size # Trains with this batch size.\n",
    "       )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "           val_dataset, # The validation samples.\n",
    "           sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "           batch_size = batch_size # Evaluate with this batch size.\n",
    "       )\n",
    "       \n",
    "#Network\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "   def __init__(self, dropout=0.1):\n",
    "\n",
    "       super(BertClassifier, self).__init__()\n",
    "\n",
    "       self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "       self.dropout = nn.Dropout(dropout)\n",
    "       self.linear = nn.Linear(768,1)\n",
    "       self.relu = nn.Sigmoid()\n",
    "\n",
    "   def forward(self, b_input_ids, b_input_mask):\n",
    "\n",
    "       _, pooled_output = self.bert(input_ids= b_input_ids, attention_mask=b_input_mask,return_dict=False)\n",
    "       dropout_output = self.dropout(pooled_output)\n",
    "       linear_output = self.linear(dropout_output)\n",
    "       final_layer = self.relu(linear_output)\n",
    "\n",
    "       return final_layer\n",
    "       \n",
    "#training method\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs, train_size, val_size):\n",
    "\n",
    "   trainlen=train_size\n",
    "   vallen=val_size\n",
    "\n",
    "   use_cuda = torch.cuda.is_available()\n",
    "   device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "   criterion = nn.BCELoss()\n",
    "   optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "   for_print = 0\n",
    "\n",
    "###############################################\n",
    "## Only if you are using gpu\n",
    "   if use_cuda:\n",
    "\n",
    "           model = model.cuda()\n",
    "           criterion = criterion.cuda()\n",
    "###############################################            \n",
    "\n",
    "   for epoch_num in range(epochs):\n",
    "\n",
    "           total_acc_train = 0\n",
    "           total_loss_train = 0\n",
    "           model.train()\n",
    "\n",
    "           for batch in tqdm(train_data):\n",
    "\n",
    "\n",
    "               b_input_ids = batch[0].to(device)\n",
    "               b_input_mask = batch[1].to(device)\n",
    "               b_labels = batch[2].to(device)\n",
    "\n",
    "               # this is resetting the model parameters for each batch of training\n",
    "               # we want to continuously update model parameters\n",
    "               # model.zero_grad()\n",
    "               \n",
    "               optimizer.zero_grad()\n",
    "               \n",
    "               output = model(b_input_ids, b_input_mask)\n",
    "               train_pred_probs = torch.flatten(output)\n",
    "\n",
    "               batch_loss = criterion(train_pred_probs.float(), b_labels)\n",
    "\n",
    "               total_loss_train += batch_loss.item()\n",
    "\n",
    "               acc = (train_pred_probs.round() == b_labels).sum().item()\n",
    "               total_acc_train += acc\n",
    "                \n",
    "               batch_loss.backward() \n",
    "               optimizer.step()\n",
    "           \n",
    "           total_acc_val = 0\n",
    "           total_loss_val = 0\n",
    "\n",
    "           model.eval()\n",
    "\n",
    "           with torch.no_grad():\n",
    "               for batch in val_data:\n",
    "               \n",
    "                   b_input_ids = batch[0].to(device)\n",
    "                   b_input_mask = batch[1].to(device)\n",
    "                   b_labels = batch[2].to(device)\n",
    "\n",
    "\n",
    "                   output = model(b_input_ids,b_input_mask)\n",
    "                   val_pred_probs = torch.flatten(output)\n",
    "\n",
    "                   batch_loss = criterion(val_pred_probs.float(), b_labels)\n",
    "                   total_loss_val += batch_loss.item()\n",
    "                   acc = (val_pred_probs.round() == b_labels).sum().item()\n",
    "                   total_acc_val += acc\n",
    "           \n",
    "           print(\n",
    "               f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / trainlen : .3f} \\\n",
    "               | Train Accuracy: {total_acc_train / trainlen : .3f} \\\n",
    "               | {total_acc_train} and {trainlen}\\\n",
    "               | Val Loss: {total_loss_val / vallen: .3f} \\\n",
    "               | Val Accuracy: {total_acc_val / vallen: .3f}\\\n",
    "               | {total_acc_val} and {vallen}'\n",
    "               )\n",
    "                 \n",
    "EPOCHS = 4\n",
    "model = BertClassifier()\n",
    "LR = 2e-5\n",
    "             \n",
    "train(model,train_dataloader, validation_dataloader, LR, EPOCHS, train_size, val_size)\n",
    "\n",
    "torch.save(model.state_dict(), 'Path to the folder where you want to save the weights')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529b9b4",
   "metadata": {},
   "source": [
    "## Testing\n",
    "We should carry out all data preprocessing tasks the same way you did for training data. And in test dataset there will be no labels only tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importing all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import sys\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#Loading the data\n",
    "TestData=pd.read_csv(\"Give path to the csv test dataset file you downloaded\")\n",
    "Data=TestData.iloc[:,1:](Keeping only tweets)\n",
    "Data\n",
    "\n",
    "#Checking and removing null values\n",
    "for col in Data.columns:\n",
    "   print(col, Data[col].isnull().sum())\n",
    "Data= Data.dropna()\n",
    "\n",
    "#Now we perform some data preprocessing tasks.\n",
    "#Converting text to lowercase\n",
    "Data['clean_text'] = Data.full_text.str.lower()\n",
    "\n",
    "#Removing urls\n",
    "Data.clean_text = Data.clean_text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "Data.clean_text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x)) \n",
    "\n",
    "#Removing non-letter characters\n",
    "Data.clean_text = Data.clean_text.apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
    "Data.clean_text = Data.clean_text.apply(lambda x: re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];=\\'\\\\#\\\\]\", '', x))\n",
    "\n",
    "#Removing hashtag and quote symbols\n",
    "Data.clean_text = Data.clean_text.apply(lambda x: re.sub(r'#', '', x))\n",
    "Data.clean_text = Data.clean_text.apply(lambda x: re.sub(r\"'\", '', x))\n",
    "\n",
    "#Tokenization\n",
    "tknzr = TweetTokenizer()\n",
    "Data['clean_text'] = Data['clean_text'].apply(tknzr.tokenize)\n",
    "\n",
    "#Removing Punctuations\n",
    "PUNCUATION_LIST = list(string.punctuation)\n",
    "def remove_punctuation(word_list):\n",
    "   \"\"\"Remove punctuation tokens from a list of tokens\"\"\"\n",
    "   return [w for w in word_list if w not in PUNCUATION_LIST]\n",
    "Data['clean_text'] = Data['clean_text'].apply(remove_punctuation)\n",
    "\n",
    "#Removing Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "Data['clean_text'] = Data['clean_text'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "#Performing Lemmatization\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = TweetTokenizer()\n",
    "def lemmatize_text(text):\n",
    "return [(lemmatizer.lemmatize(w)) for w in text]\n",
    "Data['clean_text'] = Data['clean_text'].apply(lemmatize_text)\n",
    "\n",
    "#Putting tokens back into string\n",
    "Data['clean_text']=[' '.join(map(str,l)) for l in Data['clean_text']]\n",
    "\n",
    "#Labeling original as '0' and Fake as '1'\n",
    "Data.Y[Data.Y == 'Original'] = 0\n",
    "Data.Y[Data.Y == 'Fake'] = 1\n",
    "TrainData=Data.iloc[:,1:]\n",
    "\n",
    "# Get the lists of sentences.\n",
    "sentences = Traindata.clean_text.values\n",
    "\n",
    "#Importing the Bert Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "   # `encode_plus` will:\n",
    "   #   (1) Tokenize the sentence.\n",
    "   #   (2) Prepend the `[CLS]` token to the start.\n",
    "   #   (3) Append the `[SEP]` token to the end.\n",
    "   #   (4) Map tokens to their IDs.\n",
    "   #   (5) Pad or truncate the sentence to `max_length`\n",
    "   #   (6) Create attention masks for [PAD] tokens.\n",
    "   encoded_dict = tokenizer.encode_plus(\n",
    "                       sent,                      # Sentence to encode.\n",
    "                       add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                       max_length = 64,           # Pad & truncate all sentences.\n",
    "                       pad_to_max_length = True,\n",
    "                       return_attention_mask = True,   # Construct attn. masks.\n",
    "                       return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                  )\n",
    "   \n",
    "   # Add the encoded sentence to the list.    \n",
    "   input_ids.append(encoded_dict['input_ids'])\n",
    "   \n",
    "   # And its attention mask (simply differentiates padding from non-padding).\n",
    "   attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Combine the testing inputs into a TensorDataset.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "prediction_sampler = RandomSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "#Network\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "   def __init__(self, dropout=0.1):\n",
    "\n",
    "       super(BertClassifier, self).__init__()\n",
    "\n",
    "       self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "       self.dropout = nn.Dropout(dropout)\n",
    "       self.linear = nn.Linear(768,1)\n",
    "       self.relu = nn.Sigmoid()\n",
    "\n",
    "   def forward(self, b_input_ids, b_input_mask):\n",
    "\n",
    "       _, pooled_output = self.bert(input_ids= b_input_ids, attention_mask=b_input_mask,return_dict=False)\n",
    "       dropout_output = self.dropout(pooled_output)\n",
    "       linear_output = self.linear(dropout_output)\n",
    "       final_layer = self.relu(linear_output)\n",
    "\n",
    "       return final_layer\n",
    "       \n",
    "model = BertClassifier()\n",
    "\n",
    "#load the model\n",
    "model.load_state_dict(torch.load('Path to the file where you saved the weights'))\n",
    "       \n",
    "#Predicting\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "#############################################\n",
    "Only if using Gpu:\n",
    "if use_cuda:\n",
    "\n",
    "   model = model.cuda()\n",
    "#############################################\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "   for batch in prediction_dataloader:\n",
    "   \n",
    "       b_input_ids = batch[0].to(device)\n",
    "       b_input_mask = batch[1].to(device)\n",
    "\n",
    "       output = model(b_input_ids,b_input_mask)\n",
    "       test_pred_probs = torch.flatten(output)\n",
    "       labels=test_pred_probs.round()\n",
    "       out_labels=labels.detach().cpu().numpy()\n",
    "       predictions.append(out_labels)\n",
    "\n",
    "#Changing the dimensions of list \n",
    "pred_list = list(chain.from_iterable(predictions))\n",
    "\n",
    "#Adding the list to dataframe as a column\n",
    "TestData[\"Predicted Labels\"]= pred_list\n",
    "\n",
    "#converting the dataframe into submission format\n",
    "TestData.drop(['full_text'], axis=1)\n",
    "\n",
    "#converting dataframe to csv\n",
    "TestData.to_csv(\"Predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
